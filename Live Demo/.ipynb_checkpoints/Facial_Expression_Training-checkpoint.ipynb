{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=center> Facial Expression Recognition with Keras</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvGxjjeV-9Ls"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from IPython.display import SVG, Image\n",
    "from livelossplot import PlotLossesKeras\n",
    "# from livelossplot import PlotLossesTensorFlowKeras\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Plot Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "TalL_1Qr-9Qz",
    "outputId": "f5fb9b05-976a-4979-ea23-33c3d87efb94"
   },
   "outputs": [],
   "source": [
    "utils.datasets.fer.plot_example_images(plt).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for expression in os.listdir(\"train/\"):\n",
    "    print(str(len(os.listdir(\"train/\"+expression))) + \" \" + expression + \" images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Generate Training and Validation Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "iri8ehFw-9Tj",
    "outputId": "1cff3826-c0a9-41ff-a61b-5a677de101ae"
   },
   "outputs": [],
   "source": [
    "img_size = 48\n",
    "batch_size = 64\n",
    "\n",
    "# Data generator to augment data for training\n",
    "datagen_train = ImageDataGenerator(horizontal_flip=True)\n",
    "train_generator = datagen_train.flow_from_directory(\"train/\", \n",
    "                                                    target_size=(img_size,img_size), \n",
    "                                                    color_mode='grayscale',\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode='categorical',\n",
    "                                                   shuffle=True)\n",
    "\n",
    "# Data generator to augment data for validation\n",
    "datagen_validation = ImageDataGenerator(horizontal_flip=True)\n",
    "validation_generator = datagen_train.flow_from_directory(\"test/\", \n",
    "                                                    target_size=(img_size,img_size), \n",
    "                                                    color_mode='grayscale',\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode='categorical',\n",
    "                                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Create CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model.png)\n",
    "Inspired by Goodfellow, I.J., et.al. (2013). Challenged in representation learning: A report of three machine learning contests. *Neural Networks*, 64, 59-63. [doi:10.1016/j.neunet.2014.09.005](https://arxiv.org/pdf/1307.0414.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Conv Block 1\n",
    "model.add(Conv2D(64, (3,3), padding='same', input_shape=(48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Conv Block 2\n",
    "model.add(Conv2D(128,(5,5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Conv Block 3\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Conv Block 3\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected Block 1\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully connected Block 2\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "steps_per_epoch= train_generator.n//train_generator.batch_size\n",
    "validation_steps = validation_generator.n//validation_generator.batch_size\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\",monitor='val_accuracy',\n",
    "                            save_weights_only=True, mode='max',verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss' , factor=0.1, patience=2, min_lr=0.00001,model='auto')\n",
    "\n",
    "callbacks = [PlotLossesKeras(), checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit(\n",
    "        x= train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Represent Model as JSON String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHw8ir7CVAE0"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [27/Aug/2020 00:24:38] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [27/Aug/2020 00:24:39] \"\u001b[37mGET /video_feed HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [27/Aug/2020 00:24:45] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [27/Aug/2020 00:24:46] \"\u001b[37mGET /video_feed HTTP/1.1\u001b[0m\" 200 -\n",
      "Error on request:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nischal\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\werkzeug\\serving.py\", line 306, in run_wsgi\n",
      "    execute(self.server.app)\n",
      "  File \"C:\\Users\\Nischal\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\werkzeug\\serving.py\", line 296, in execute\n",
      "    for data in application_iter:\n",
      "  File \"C:\\Users\\Nischal\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\werkzeug\\wsgi.py\", line 506, in __next__\n",
      "    return self._next()\n",
      "  File \"C:\\Users\\Nischal\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\werkzeug\\wrappers\\base_response.py\", line 45, in _iter_encoded\n",
      "    for item in iterable:\n",
      "  File \"<ipython-input-2-f3b699a30d4f>\", line 13, in gen\n",
      "    \n",
      "  File \"C:\\Users\\Nischal\\Desktop\\Perception\\Facial Expression Project\\camera.py\", line 20, in get_frame\n",
      "    gray_fr = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
      "cv2.error: OpenCV(4.4.0) ..\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "127.0.0.1 - - [27/Aug/2020 00:24:54] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [27/Aug/2020 00:24:55] \"\u001b[37mGET /video_feed HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, Response\n",
    "from camera import VideoCamera\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index1.html')\n",
    "\n",
    "def gen(camera):\n",
    "    while True:\n",
    "        frame = camera.get_frame()\n",
    "        yield (b'--frame\\r\\n'\n",
    "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n\\r\\n')\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(gen(VideoCamera()),\n",
    "                    mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='localhost', debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Facial_Expression_Training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
